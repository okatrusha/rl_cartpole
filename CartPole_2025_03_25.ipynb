{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTyTyvPIaAF5"
      },
      "source": [
        "### OpenAI CartPole https://gymnasium.farama.org/environments/classic_control/cart_pole/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2gNwNrYwZC"
      },
      "source": [
        "## Install dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWqhZ3XqYuki"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Obtaining dependency information for gymnasium from https://files.pythonhosted.org/packages/f9/68/2bdc7b46b5f543dd865575f9d19716866bdb76e50dd33b71ed1a3dd8bb42/gymnasium-1.1.1-py3-none-any.whl.metadata\n",
            "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting numpy>=1.21.0 (from gymnasium)\n",
            "  Obtaining dependency information for numpy>=1.21.0 from https://files.pythonhosted.org/packages/98/89/0c93baaf0094bdaaaa0536fe61a27b1dce8a505fa262a865ec142208cfe9/numpy-2.2.5-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading numpy-2.2.5-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
            "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
            "     ------------------------- ------------ 41.0/60.8 kB 991.0 kB/s eta 0:00:01\n",
            "     -------------------------------------- 60.8/60.8 kB 816.2 kB/s eta 0:00:00\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
            "  Obtaining dependency information for cloudpickle>=1.2.0 from https://files.pythonhosted.org/packages/7e/e8/64c37fadfc2816a7701fa8a6ed8d87327c7d54eacfbfb6edab14a2f2be75/cloudpickle-3.1.1-py3-none-any.whl.metadata\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\asp\\rl\\vnev\\lib\\site-packages (from gymnasium) (4.13.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Obtaining dependency information for farama-notifications>=0.0.1 from https://files.pythonhosted.org/packages/05/2c/ffc08c54c05cdce6fbed2aeebc46348dbe180c6d2c541c7af7ba0aa5f5f8/Farama_Notifications-0.0.4-py3-none-any.whl.metadata\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
            "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
            "   -- ------------------------------------- 71.7/965.4 kB 2.0 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 204.8/965.4 kB 2.5 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 399.4/965.4 kB 3.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 686.1/965.4 kB 4.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 965.4/965.4 kB 5.1 MB/s eta 0:00:00\n",
            "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading numpy-2.2.5-cp311-cp311-win_amd64.whl (12.9 MB)\n",
            "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/12.9 MB 13.8 MB/s eta 0:00:01\n",
            "   - -------------------------------------- 0.5/12.9 MB 10.2 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.7/12.9 MB 6.0 MB/s eta 0:00:03\n",
            "   -- ------------------------------------- 0.8/12.9 MB 5.3 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 1.0/12.9 MB 5.3 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 1.2/12.9 MB 5.2 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 1.4/12.9 MB 5.4 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 1.6/12.9 MB 5.0 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 1.8/12.9 MB 5.0 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 2.2/12.9 MB 5.7 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 2.4/12.9 MB 5.8 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 2.4/12.9 MB 5.2 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 3.4/12.9 MB 6.7 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.4/12.9 MB 6.4 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.4/12.9 MB 5.9 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 3.6/12.9 MB 6.0 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 3.8/12.9 MB 5.8 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 4.4/12.9 MB 6.2 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 4.8/12.9 MB 6.5 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 5.2/12.9 MB 6.7 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 5.6/12.9 MB 6.9 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 6.1/12.9 MB 7.0 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 6.5/12.9 MB 7.3 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 6.9/12.9 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 7.2/12.9 MB 7.3 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 7.7/12.9 MB 7.6 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 8.0/12.9 MB 7.5 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 8.5/12.9 MB 7.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 8.9/12.9 MB 7.8 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 9.2/12.9 MB 7.9 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 9.7/12.9 MB 8.0 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 10.2/12.9 MB 8.2 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 10.5/12.9 MB 8.0 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 11.0/12.9 MB 8.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 11.4/12.9 MB 8.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.7/12.9 MB 9.1 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 12.1/12.9 MB 9.5 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.5/12.9 MB 9.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.9/12.9 MB 9.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.9/12.9 MB 9.5 MB/s eta 0:00:00\n",
            "Installing collected packages: farama-notifications, numpy, cloudpickle, gymnasium\n",
            "Successfully installed cloudpickle-3.1.1 farama-notifications-0.0.4 gymnasium-1.1.1 numpy-2.2.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # CartPole Reinforcement Learning in Colab\n",
        "#\n",
        "# This notebook demonstrates how to train an agent to balance the CartPole using Deep Q-Learning.\n",
        "\n",
        "# %%\n",
        "!pip install gymnasium \n",
        "!pip install tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwNQDwCAYuQl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO9PiGrUZV_Y"
      },
      "source": [
        "# 1. Ініціалізація середовища\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHai1Jl9oqGP"
      },
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  \n",
        "\n",
        "Для візуалізації у вікні\n",
        "\n",
        "або\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  \n",
        "\n",
        "Для отримання кадру як масиву numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNPA4o6gZZCu",
        "outputId": "fc8faf17-984d-4574-d445-0e49560ee128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State size: 4, Action size: 2\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "state_size = env.observation_space.shape[0]  # 4 змінні: позиція, швидкість, кут, кутова швидкість\n",
        "action_size = env.action_space.n  # 2 дії: рух вліво (0) або вправо (1)\n",
        "print(f\"State size: {state_size}, Action size: {action_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrz6iOu_aNzP"
      },
      "source": [
        "CartPole-v1 — середовище, де агент керує візком, щоб утримувати стрижень у вертикальному положенні.\n",
        "\n",
        "Стан (state): 4 числа:\n",
        "\n",
        "Позиція візка (x),\n",
        "\n",
        "Швидкість візка (v),\n",
        "\n",
        "Кут стрижня (θ),\n",
        "\n",
        "Кутова швидкість стрижня (ω).\n",
        "\n",
        "Дії (actions): 0 (ліворуч) або 1 (праворуч)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQwO88QaTe_"
      },
      "source": [
        "## 2. Побудова нейромережі\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omu-7YMrag0J"
      },
      "outputs": [],
      "source": [
        "# Deep Q-Network model\n",
        "def build_model(state_size, action_size):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(24, input_dim=state_size, activation='relu'),\n",
        "        keras.layers.Dense(24, activation='relu'),\n",
        "        keras.layers.Dense(action_size, activation='linear')\n",
        "    ])\n",
        "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocG_4c88ckRa"
      },
      "source": [
        "Архітектура нейромережі для DQN у задачі CartPole обрана не випадково — вона ґрунтується на поєднанні теоретичних принципів глибокого навчання та практичного досвіду. Розберімо чому саме такий дизайн:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsIvTsY9ckOP"
      },
      "source": [
        "1. **Вхідний шар (4 нейрони)**\n",
        "\n",
        "*   **Чому 4?**\n",
        "\n",
        "Вхідний шар відповідає розмірності стану середовища CartPole:\n",
        "[позиція візка, швидкість візка, кут стрижня, кутова швидкість стрижня].\n",
        "Кожен нейрон приймає одну з цих змінних.\n",
        "\n",
        "*   **Чому немає додаткової обробки?**\n",
        "\n",
        "Дані вже нормалізовані (на відміну, наприклад, від зображень), тому не потрібні складні шари (на кшталт згорткових)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1IN9AUrckLX"
      },
      "source": [
        "**2. Приховані шари (2 шари по 24 нейрони з ReLU)**\n",
        "\n",
        "* **Чому 2 шари?**\n",
        "\n",
        "**Перший** шар виділяє прості ознаки (наприклад, залежність кута від швидкості).\n",
        "\n",
        "**Другий** шар комбінує їх у складніші паттерни (наприклад, \"якщо стрижень відхиляється вліво і рухається швидко, потрібно їхати вліво\").\n",
        "Глибші шари (3+) рідко дають виграш для таких простих задач.\n",
        "\n",
        "* **Чому 24 нейрони?**\n",
        "\n",
        "Емпіричне правило для DQN:\n",
        "\n",
        "Занадто мало нейронів (наприклад, 8) → мережа не зможе навчитися складним залежностям.\n",
        "\n",
        "Занадто багато (наприклад, 128) → ризик перенавчання або повільного навчання.\n",
        "**24 — компроміс між швидкістю та якістю для CartPole.**\n",
        "\n",
        "* **Чому ReLU?**\n",
        "\n",
        "* * **Переваги ReLU:**\n",
        "\n",
        "* * * Нелінійність (дозволяє навчатися складним функціям).\n",
        "\n",
        "* * * Швидкі обчислення (порівняно з Tanh/Sigmoid).\n",
        "\n",
        "* * * Уникає проблеми \"зникаючих градієнтів\" (як у Sigmoid).\n",
        "\n",
        "* * Для Q-навчання критично мати нелінійність, щоб апроксимувати складні Q-функції."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg4m24PTckIb"
      },
      "source": [
        "**3. Вихідний шар** (2 нейрони з лінійною активацією)\n",
        "\n",
        "* **Чому 2 нейрони?**\n",
        "\n",
        "Кількість дій у CartPole — 2 (ліворуч/праворуч). Кожен нейрон виводить Q-значення для відповідної дії.\n",
        "\n",
        "* **Чому лінійна активація?**\n",
        "\n",
        "Q-значення можуть бути будь-якими числами (від -∞ до +∞), тому лінійна активація (без обмежень) ідеальна для регресії."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlykGGjTckFb"
      },
      "source": [
        "**4. Функція втрат (MSE)**\n",
        "\n",
        "* **Чому MSE?**\n",
        "\n",
        "Завдання — передбачити Q-значення (регресія). MSE карає за великі відхилення від цільових значень (Bellman targets):\n",
        "\n",
        "$$\n",
        "loss = (Q_{predicted} - (reward + γ * max(Q_{target}))^2\n",
        "$$\n",
        "\n",
        "Альтернативи (наприклад, Huber loss) менш чутливі до викидів, але MSE часто працює стабільніше для DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ6cC8gHckCg"
      },
      "source": [
        "**5. Оптимізатор** (Adam з lr=0.001)\n",
        "\n",
        "* **Чому Adam?**\n",
        "\n",
        "* * Адаптує learning rate для кожного параметра (краще, ніж SGD).\n",
        "\n",
        "* * Стабілізує навчання за рахунок моментуму.\n",
        "\n",
        "* * Працює краще за RMSprop для DQN у більшості досліджень.\n",
        "\n",
        "* **Чому lr=0.001?**\n",
        "\n",
        "* * Занадто великий (наприклад, 0.01) → навчання нестабільне.\n",
        "\n",
        "* * Занадто малий (наприклад, 0.0001) → повільне навчання.\n",
        "\n",
        "* * 0.001 — стандартне значення для багатьох RL-задач."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBwzhp8cj8r"
      },
      "source": [
        "**Емпіричне підтвердження**\n",
        "\n",
        "Така архітектура стала де-факто стандартом після публікацій DeepMind (наприклад, Human-level control through deep reinforcement learning):\n",
        "\n",
        "* Для простіших задач (як CartPole) достатньо 2-х прихованих шарів.\n",
        "\n",
        "* Більше нейронів (24-64) дає кращу апроксимацію Q-функції, але потребує більше даних.\n",
        "\n",
        "* ReLU + Adam — найефективніше поєднання для швидкого навчання."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18_rt4Ygcjy_"
      },
      "source": [
        "**Що буде, якщо змінити архітектуру?**\n",
        "\n",
        "Зміна\t| Ефект\n",
        "\n",
        "1 прихований шар |\tМережа може не навчитися складним стратегіям (наприклад, балансування при великій швидкості).\n",
        "\n",
        "Більше 2 шарів | Ризик перенавчання або повільного навчання без реального виграшу.\n",
        "\n",
        "Sigmoid замість ReLU | Повільне навчання через \"зникаючі градієнти\".\n",
        "Більше нейронів (наприклад, 128)\tМоже покращити якість, але вимагає більше даних і часу.\n",
        "\n",
        "Менше нейронів (наприклад, 8) |\tМережа не зможе знайти оптимальну стратегію."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SweA7tBCamHu"
      },
      "source": [
        "Для CartPole запропонована архітектура — це оптимальний баланс між простотою та ефективністю. Для складніших задач (наприклад, Atari) використовують більші мережі (згорткові + повнозв’язні шари)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbn49m2vflcN"
      },
      "source": [
        "**Математичний опис архітектури DQN для CartPole**\n",
        "\n",
        "Нейромережа реалізує функцію апроксимації $Q(s,a;\\theta)$, де:\n",
        "\n",
        "* $s \\in \\mathbb{R}^4$ - стан середовища\n",
        "* $a \\in \\{0,1\\}$ - дія\n",
        "* $\\theta$ - параметри мережі\n",
        "\n",
        "\n",
        "**1. Вхідний шар**\n",
        "\n",
        "Приймає вектор стану:\n",
        "$$ \\mathbf{x}^{(0)} = \\mathbf{s} $$\n",
        "\n",
        "**2. Перший прихований шар**\n",
        "\\begin{align*}\n",
        "    \\mathbf{z}^{(1)} &= \\mathbf{W}^{(1)}\\mathbf{x}^{(0)} + \\mathbf{b}^{(1)} \\\\\n",
        "    & \\text{де } \\mathbf{W}^{(1)} \\in \\mathbb{R}^{24 \\times 4}, \\mathbf{b}^{(1)} \\in \\mathbb{R}^{24} \\\\\n",
        "    \\mathbf{x}^{(1)} &= \\text{ReLU}(\\mathbf{z}^{(1)}) = \\max(0, \\mathbf{z}^{(1)})\n",
        "\\end{align*}\n",
        "\n",
        "**3. Другий прихований шар**\n",
        "\\begin{align*}\n",
        "    \\mathbf{z}^{(2)} &= \\mathbf{W}^{(2)}\\mathbf{x}^{(1)} + \\mathbf{b}^{(2)} \\\\\n",
        "    & \\text{де } \\mathbf{W}^{(2)} \\in \\mathbb{R}^{24 \\times 24}, \\mathbf{b}^{(2)} \\in \\mathbb{R}^{24} \\\\\n",
        "    \\mathbf{x}^{(2)} &= \\text{ReLU}(\\mathbf{z}^{(2)})\n",
        "\\end{align*}\n",
        "\n",
        "**4. Вихідний шар**\n",
        "\\begin{align*}\n",
        "    \\mathbf{Q}(s,\\cdot;\\theta) &= \\mathbf{W}^{(3)}\\mathbf{x}^{(2)} + \\mathbf{b}^{(3)} \\\\\n",
        "    & \\text{де } \\mathbf{W}^{(3)} \\in \\mathbb{R}^{2 \\times 24}, \\mathbf{b}^{(3)} \\in \\mathbb{R}^{2}\n",
        "\\end{align*}\n",
        "\n",
        "**Функція втрат**\n",
        "Для одного прикладу $(s,a,r,s',\\text{done})$:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) =\n",
        "\\begin{cases}\n",
        "(r - Q(s,a;\\theta))^2 & \\text{якщо done = True} \\\\\n",
        "(r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta))^2 & \\text{інакше}\n",
        "\\end{cases}\n",
        "$$\n",
        "де $\\gamma = 0.95$ - коефіцієнт дисконтування, $\\theta^-$ - параметри цільової мережі.\n",
        "\n",
        "**Повний forward pass**\n",
        "Для батчу станів $\\mathbf{S} \\in \\mathbb{R}^{\\text{batch\\_size} \\times 4}$:\n",
        "$$\n",
        "\\mathbf{Q}(\\mathbf{S};\\theta) = f_{\\theta}^{(3)} \\circ f_{\\theta}^{(2)} \\circ f_{\\theta}^{(1)}(\\mathbf{S})\n",
        "$$\n",
        "де:\n",
        "\\begin{align*}\n",
        "    f_{\\theta}^{(1)}(\\mathbf{x}) &= \\text{ReLU}(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}) \\\\\n",
        "    f_{\\theta}^{(2)}(\\mathbf{x}) &= \\text{ReLU}(\\mathbf{W}^{(2)}\\mathbf{x} + \\mathbf{b}^{(2)}) \\\\\n",
        "    f_{\\theta}^{(3)}(\\mathbf{x}) &= \\mathbf{W}^{(3)}\\mathbf{x} + \\mathbf{b}^{(3)}\n",
        "\\end{align*}\n",
        "\n",
        "**Оновлення параметрів**\n",
        "Використовується оптимізатор Adam:\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\alpha_t \\hat{\\mathbf{m}}_t / (\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon)\n",
        "$$\n",
        "де $\\alpha_t = 0.001$, $\\hat{\\mathbf{m}}_t$ і $\\hat{\\mathbf{v}}_t$ - коректені оцінки першого та другого моментів."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xigfY9p7g9zu"
      },
      "source": [
        "Ця архітектура є компромісом між:\n",
        "\n",
        " * **Ємністю моделі** (здатністю апроксимувати складну Q-функцію)\n",
        "\n",
        " * **Швидкістю навчання** (обмежена кількість параметрів)\n",
        "\n",
        " * **Стабільністю** (використання ReLU та Adam для уникнення проблем з градієнтами)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWvfZkzhYvR"
      },
      "source": [
        "**3. Алгоритм DQN агента**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG4dA6tVh0As"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "\n",
        "    # Ініціалізація:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000) # Булфер досвіду (experience replay)\n",
        "        self.gamma = 0.95    # Коефіцієнт дисконтування майбутніх нагород\n",
        "        self.epsilon = 1.0    # Початкова ймовірність випадкової дії (exploration)\n",
        "        self.epsilon_min = 0.01 # Мінімальний epsilon\n",
        "        self.epsilon_decay = 0.995 # Швидкість зменшення epsilon\n",
        "        self.model = build_model(state_size, action_size)\n",
        "        self.target_model = build_model(state_size, action_size)\n",
        "        self.update_target_model()\n",
        "\n",
        "      # Experience Replay: Зберігає попередні досвіди (state, action, reward, next_state, done) для навчання.\n",
        "      # Gamma: Визначає важливість майбутніх нагород (0.95 = агент планує на 20 кроків уперед).\n",
        "      # Epsilon-Greedy: Спочатку агент досліджує (випадкові дії), потім все більше експлуатує знання.\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    # Зберігання досвіду (remember):\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "      # Додає кортеж (state, action, reward, next_state, done) до буфера memory.\n",
        "\n",
        "    # Вибір дії (act):\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "     # Якщо epsilon = 0.1, то в 10% випадків дія випадкова, в 90% — обрана на основі Q-значень.\n",
        "\n",
        "    # Навчання на досвіді (replay):\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size) # Вибірка 32 випадкових досвідів\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = self.model.predict(state, verbose=0) # Поточні прогнози Q-значень\n",
        "            if done:\n",
        "                target[0][action] = reward # Якщо епізод закінчено, Q = reward\n",
        "            else:\n",
        "                t = self.target_model.predict(next_state, verbose=0)\n",
        "                target[0][action] = reward + self.gamma * np.amax(t[0]) # Bellman equation\n",
        "            self.model.fit(state, target, epochs=1, verbose=0) # Корекція ваг\n",
        "        # Політика epsilon-greedy\n",
        "        # Епсилон зменшується після кожного навчання:\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "          # Результат:\n",
        "            # Спочатку агент досліджує (випадкові дії), потім переходить до експлуатації знань.\n",
        "\n",
        "        # Bellman Equation:\n",
        "        #Q(s,a) = reward + γ * max(Q(s',a'))\n",
        "        #де s' — наступний стан, a' — найкраща дія в ньому.\n",
        "\n",
        "        #Target Model: Допомагає стабілізувати навчання, використовуючи окрему мережу для прогнозування Q(s',a').\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v52x-_ljyEG"
      },
      "source": [
        "4. Цикл навчання"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngti_49jjxmq",
        "outputId": "2e3205cd-84c7-49a3-e86d-2248d556fb81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Training parameters\n",
        "EPISODES = 700\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "done = False\n",
        "scores = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peNH8JszXURl",
        "outputId": "12007e1c-2c6d-445b-f0b1-fcee49ff64ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/700, score: 13, e: 1.00\n",
            "episode: 1/700, score: 22, e: 1.00\n",
            "episode: 2/700, score: 17, e: 0.99\n",
            "episode: 3/700, score: 16, e: 0.99\n",
            "episode: 4/700, score: 14, e: 0.99\n",
            "episode: 5/700, score: 16, e: 0.98\n",
            "episode: 6/700, score: 11, e: 0.98\n",
            "episode: 7/700, score: 16, e: 0.97\n",
            "episode: 8/700, score: 21, e: 0.97\n",
            "episode: 9/700, score: 12, e: 0.96\n",
            "episode: 10/700, score: 9, e: 0.96\n",
            "episode: 11/700, score: 13, e: 0.95\n",
            "episode: 12/700, score: 15, e: 0.95\n",
            "episode: 13/700, score: 33, e: 0.94\n",
            "episode: 14/700, score: 13, e: 0.94\n",
            "episode: 15/700, score: 9, e: 0.93\n",
            "episode: 16/700, score: 12, e: 0.93\n",
            "episode: 17/700, score: 29, e: 0.92\n",
            "episode: 18/700, score: 20, e: 0.92\n",
            "episode: 19/700, score: 10, e: 0.91\n",
            "episode: 20/700, score: 12, e: 0.91\n",
            "episode: 21/700, score: 16, e: 0.90\n",
            "episode: 22/700, score: 50, e: 0.90\n",
            "episode: 23/700, score: 36, e: 0.90\n",
            "episode: 24/700, score: 28, e: 0.89\n",
            "episode: 25/700, score: 32, e: 0.89\n",
            "episode: 26/700, score: 38, e: 0.88\n",
            "episode: 27/700, score: 36, e: 0.88\n",
            "episode: 28/700, score: 13, e: 0.87\n",
            "episode: 29/700, score: 24, e: 0.87\n",
            "episode: 30/700, score: 12, e: 0.86\n",
            "episode: 31/700, score: 25, e: 0.86\n",
            "episode: 32/700, score: 12, e: 0.86\n",
            "episode: 33/700, score: 13, e: 0.85\n",
            "episode: 34/700, score: 15, e: 0.85\n",
            "episode: 35/700, score: 11, e: 0.84\n",
            "episode: 36/700, score: 8, e: 0.84\n",
            "episode: 37/700, score: 15, e: 0.83\n",
            "episode: 38/700, score: 28, e: 0.83\n",
            "episode: 39/700, score: 14, e: 0.83\n",
            "episode: 40/700, score: 69, e: 0.82\n",
            "episode: 41/700, score: 18, e: 0.82\n",
            "episode: 42/700, score: 13, e: 0.81\n",
            "episode: 43/700, score: 12, e: 0.81\n",
            "episode: 44/700, score: 9, e: 0.81\n",
            "episode: 45/700, score: 14, e: 0.80\n",
            "episode: 46/700, score: 9, e: 0.80\n",
            "episode: 47/700, score: 19, e: 0.79\n",
            "episode: 48/700, score: 11, e: 0.79\n",
            "episode: 49/700, score: 42, e: 0.79\n",
            "episode: 50/700, score: 13, e: 0.78\n",
            "episode: 51/700, score: 8, e: 0.78\n",
            "episode: 52/700, score: 21, e: 0.77\n",
            "episode: 53/700, score: 11, e: 0.77\n",
            "episode: 54/700, score: 15, e: 0.77\n",
            "episode: 55/700, score: 24, e: 0.76\n",
            "episode: 56/700, score: 20, e: 0.76\n",
            "episode: 57/700, score: 35, e: 0.76\n",
            "episode: 58/700, score: 32, e: 0.75\n",
            "episode: 59/700, score: 14, e: 0.75\n",
            "episode: 60/700, score: 39, e: 0.74\n",
            "episode: 61/700, score: 11, e: 0.74\n",
            "episode: 62/700, score: 38, e: 0.74\n",
            "episode: 63/700, score: 29, e: 0.73\n",
            "episode: 64/700, score: 14, e: 0.73\n",
            "episode: 65/700, score: 19, e: 0.73\n",
            "episode: 66/700, score: 79, e: 0.72\n",
            "episode: 67/700, score: 95, e: 0.72\n",
            "episode: 68/700, score: 92, e: 0.71\n",
            "episode: 69/700, score: 58, e: 0.71\n",
            "episode: 70/700, score: 12, e: 0.71\n",
            "episode: 71/700, score: 14, e: 0.70\n",
            "episode: 72/700, score: 12, e: 0.70\n",
            "episode: 73/700, score: 14, e: 0.70\n",
            "episode: 74/700, score: 14, e: 0.69\n",
            "episode: 75/700, score: 10, e: 0.69\n",
            "episode: 76/700, score: 17, e: 0.69\n",
            "episode: 77/700, score: 15, e: 0.68\n",
            "episode: 78/700, score: 12, e: 0.68\n",
            "episode: 79/700, score: 19, e: 0.68\n",
            "episode: 80/700, score: 18, e: 0.67\n",
            "episode: 81/700, score: 100, e: 0.67\n",
            "episode: 82/700, score: 28, e: 0.67\n",
            "episode: 83/700, score: 21, e: 0.66\n",
            "episode: 84/700, score: 32, e: 0.66\n",
            "episode: 85/700, score: 55, e: 0.66\n",
            "episode: 86/700, score: 24, e: 0.65\n",
            "episode: 87/700, score: 25, e: 0.65\n",
            "episode: 88/700, score: 62, e: 0.65\n",
            "episode: 89/700, score: 11, e: 0.64\n",
            "episode: 90/700, score: 36, e: 0.64\n",
            "episode: 91/700, score: 44, e: 0.64\n",
            "episode: 92/700, score: 16, e: 0.63\n",
            "episode: 93/700, score: 22, e: 0.63\n",
            "episode: 94/700, score: 55, e: 0.63\n",
            "episode: 95/700, score: 24, e: 0.62\n",
            "episode: 96/700, score: 32, e: 0.62\n",
            "episode: 97/700, score: 25, e: 0.62\n",
            "episode: 98/700, score: 11, e: 0.61\n",
            "episode: 99/700, score: 27, e: 0.61\n",
            "episode: 100/700, score: 15, e: 0.61\n",
            "episode: 101/700, score: 27, e: 0.61\n",
            "episode: 102/700, score: 33, e: 0.60\n",
            "episode: 103/700, score: 32, e: 0.60\n",
            "episode: 104/700, score: 24, e: 0.60\n",
            "episode: 105/700, score: 13, e: 0.59\n",
            "episode: 106/700, score: 59, e: 0.59\n",
            "episode: 107/700, score: 23, e: 0.59\n",
            "episode: 108/700, score: 32, e: 0.58\n",
            "episode: 109/700, score: 26, e: 0.58\n",
            "episode: 110/700, score: 13, e: 0.58\n",
            "episode: 111/700, score: 16, e: 0.58\n",
            "episode: 112/700, score: 22, e: 0.57\n",
            "episode: 113/700, score: 16, e: 0.57\n",
            "episode: 114/700, score: 56, e: 0.57\n",
            "episode: 115/700, score: 28, e: 0.56\n",
            "episode: 116/700, score: 64, e: 0.56\n",
            "episode: 117/700, score: 33, e: 0.56\n",
            "episode: 118/700, score: 47, e: 0.56\n",
            "episode: 119/700, score: 102, e: 0.55\n",
            "episode: 120/700, score: 41, e: 0.55\n",
            "episode: 121/700, score: 28, e: 0.55\n",
            "episode: 122/700, score: 134, e: 0.55\n",
            "episode: 123/700, score: 26, e: 0.54\n",
            "episode: 124/700, score: 10, e: 0.54\n",
            "episode: 125/700, score: 13, e: 0.54\n",
            "episode: 126/700, score: 49, e: 0.53\n",
            "episode: 127/700, score: 14, e: 0.53\n",
            "episode: 128/700, score: 29, e: 0.53\n",
            "episode: 129/700, score: 79, e: 0.53\n",
            "episode: 130/700, score: 50, e: 0.52\n",
            "episode: 131/700, score: 69, e: 0.52\n",
            "episode: 132/700, score: 55, e: 0.52\n",
            "episode: 133/700, score: 30, e: 0.52\n",
            "episode: 134/700, score: 56, e: 0.51\n",
            "episode: 135/700, score: 109, e: 0.51\n",
            "episode: 136/700, score: 32, e: 0.51\n",
            "episode: 137/700, score: 44, e: 0.51\n",
            "episode: 138/700, score: 28, e: 0.50\n",
            "episode: 139/700, score: 46, e: 0.50\n",
            "episode: 140/700, score: 47, e: 0.50\n",
            "episode: 141/700, score: 12, e: 0.50\n",
            "episode: 142/700, score: 24, e: 0.49\n",
            "episode: 143/700, score: 53, e: 0.49\n",
            "episode: 144/700, score: 28, e: 0.49\n",
            "episode: 145/700, score: 22, e: 0.49\n",
            "episode: 146/700, score: 71, e: 0.48\n",
            "episode: 147/700, score: 24, e: 0.48\n",
            "episode: 148/700, score: 14, e: 0.48\n",
            "episode: 149/700, score: 35, e: 0.48\n",
            "episode: 150/700, score: 36, e: 0.47\n",
            "episode: 151/700, score: 62, e: 0.47\n",
            "episode: 152/700, score: 64, e: 0.47\n",
            "episode: 153/700, score: 31, e: 0.47\n",
            "episode: 154/700, score: 81, e: 0.46\n",
            "episode: 155/700, score: 34, e: 0.46\n",
            "episode: 156/700, score: 32, e: 0.46\n",
            "episode: 157/700, score: 52, e: 0.46\n",
            "episode: 158/700, score: 40, e: 0.46\n",
            "episode: 159/700, score: 51, e: 0.45\n",
            "episode: 160/700, score: 46, e: 0.45\n",
            "episode: 161/700, score: 25, e: 0.45\n",
            "episode: 162/700, score: 20, e: 0.45\n",
            "episode: 163/700, score: 25, e: 0.44\n",
            "episode: 164/700, score: 19, e: 0.44\n",
            "episode: 165/700, score: 48, e: 0.44\n",
            "episode: 166/700, score: 30, e: 0.44\n",
            "episode: 167/700, score: 48, e: 0.44\n",
            "episode: 168/700, score: 42, e: 0.43\n",
            "episode: 169/700, score: 70, e: 0.43\n",
            "episode: 170/700, score: 20, e: 0.43\n",
            "episode: 171/700, score: 17, e: 0.43\n",
            "episode: 172/700, score: 26, e: 0.42\n",
            "episode: 173/700, score: 65, e: 0.42\n",
            "episode: 174/700, score: 38, e: 0.42\n",
            "episode: 175/700, score: 61, e: 0.42\n",
            "episode: 176/700, score: 42, e: 0.42\n",
            "episode: 177/700, score: 22, e: 0.41\n",
            "episode: 178/700, score: 15, e: 0.41\n",
            "episode: 179/700, score: 24, e: 0.41\n",
            "episode: 180/700, score: 30, e: 0.41\n",
            "episode: 181/700, score: 72, e: 0.41\n",
            "episode: 182/700, score: 38, e: 0.40\n",
            "episode: 183/700, score: 92, e: 0.40\n",
            "episode: 184/700, score: 67, e: 0.40\n",
            "episode: 185/700, score: 26, e: 0.40\n",
            "episode: 186/700, score: 21, e: 0.40\n",
            "episode: 187/700, score: 33, e: 0.39\n",
            "episode: 188/700, score: 93, e: 0.39\n",
            "episode: 189/700, score: 29, e: 0.39\n",
            "episode: 190/700, score: 43, e: 0.39\n",
            "episode: 191/700, score: 31, e: 0.39\n",
            "episode: 192/700, score: 57, e: 0.38\n",
            "episode: 193/700, score: 32, e: 0.38\n",
            "episode: 194/700, score: 26, e: 0.38\n",
            "episode: 195/700, score: 159, e: 0.38\n",
            "episode: 196/700, score: 74, e: 0.38\n",
            "episode: 197/700, score: 35, e: 0.37\n",
            "episode: 198/700, score: 98, e: 0.37\n",
            "episode: 199/700, score: 112, e: 0.37\n",
            "episode: 200/700, score: 19, e: 0.37\n",
            "episode: 201/700, score: 18, e: 0.37\n",
            "episode: 202/700, score: 24, e: 0.37\n",
            "episode: 203/700, score: 37, e: 0.36\n",
            "episode: 204/700, score: 71, e: 0.36\n",
            "episode: 205/700, score: 19, e: 0.36\n",
            "episode: 206/700, score: 16, e: 0.36\n",
            "episode: 207/700, score: 37, e: 0.36\n",
            "episode: 208/700, score: 64, e: 0.35\n",
            "episode: 209/700, score: 25, e: 0.35\n",
            "episode: 210/700, score: 33, e: 0.35\n",
            "episode: 211/700, score: 47, e: 0.35\n",
            "episode: 212/700, score: 55, e: 0.35\n",
            "episode: 213/700, score: 38, e: 0.35\n",
            "episode: 214/700, score: 31, e: 0.34\n",
            "episode: 215/700, score: 26, e: 0.34\n",
            "episode: 216/700, score: 36, e: 0.34\n",
            "episode: 217/700, score: 66, e: 0.34\n",
            "episode: 218/700, score: 85, e: 0.34\n",
            "episode: 219/700, score: 55, e: 0.34\n",
            "episode: 220/700, score: 195, e: 0.33\n",
            "episode: 221/700, score: 34, e: 0.33\n",
            "episode: 222/700, score: 26, e: 0.33\n",
            "episode: 223/700, score: 36, e: 0.33\n",
            "episode: 224/700, score: 34, e: 0.33\n",
            "episode: 225/700, score: 148, e: 0.33\n",
            "episode: 226/700, score: 41, e: 0.32\n",
            "episode: 227/700, score: 56, e: 0.32\n",
            "episode: 228/700, score: 36, e: 0.32\n",
            "episode: 229/700, score: 54, e: 0.32\n",
            "episode: 230/700, score: 33, e: 0.32\n",
            "episode: 231/700, score: 47, e: 0.32\n",
            "episode: 232/700, score: 40, e: 0.31\n",
            "episode: 233/700, score: 23, e: 0.31\n",
            "episode: 234/700, score: 32, e: 0.31\n",
            "episode: 235/700, score: 25, e: 0.31\n",
            "episode: 236/700, score: 84, e: 0.31\n",
            "episode: 237/700, score: 30, e: 0.31\n",
            "episode: 238/700, score: 31, e: 0.30\n",
            "episode: 239/700, score: 29, e: 0.30\n",
            "episode: 240/700, score: 28, e: 0.30\n",
            "episode: 241/700, score: 39, e: 0.30\n",
            "episode: 242/700, score: 54, e: 0.30\n",
            "episode: 243/700, score: 78, e: 0.30\n",
            "episode: 244/700, score: 87, e: 0.30\n",
            "episode: 245/700, score: 74, e: 0.29\n",
            "episode: 246/700, score: 164, e: 0.29\n",
            "episode: 247/700, score: 87, e: 0.29\n",
            "episode: 248/700, score: 78, e: 0.29\n",
            "episode: 249/700, score: 46, e: 0.29\n",
            "episode: 250/700, score: 38, e: 0.29\n",
            "episode: 251/700, score: 31, e: 0.29\n",
            "episode: 252/700, score: 53, e: 0.28\n",
            "episode: 253/700, score: 41, e: 0.28\n",
            "episode: 254/700, score: 102, e: 0.28\n",
            "episode: 255/700, score: 59, e: 0.28\n",
            "episode: 256/700, score: 138, e: 0.28\n",
            "episode: 257/700, score: 57, e: 0.28\n",
            "episode: 258/700, score: 44, e: 0.28\n",
            "episode: 259/700, score: 71, e: 0.27\n",
            "episode: 260/700, score: 73, e: 0.27\n",
            "episode: 261/700, score: 50, e: 0.27\n",
            "episode: 262/700, score: 72, e: 0.27\n",
            "episode: 263/700, score: 162, e: 0.27\n",
            "episode: 264/700, score: 91, e: 0.27\n",
            "episode: 265/700, score: 60, e: 0.27\n",
            "episode: 266/700, score: 107, e: 0.26\n",
            "episode: 267/700, score: 94, e: 0.26\n",
            "episode: 268/700, score: 64, e: 0.26\n",
            "episode: 269/700, score: 145, e: 0.26\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Training loop\n",
        "for e in range(EPISODES):\n",
        "    # Скидання середовища:\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):\n",
        "        # Uncomment to render (slows down training)\n",
        "        env.render()\n",
        "        #Крок агента:\n",
        "          #Обрати дію (act),\n",
        "          #виконати її в середовищі (env.step).\n",
        "        # Після навчання агент виконує дії на основі Q-значень без досліджень (epsilon = 0):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            # Оновлення цільової мережі:\n",
        "            agent.update_target_model()\n",
        "            print(f\"episode: {e}/{EPISODES}, score: {time}, e: {agent.epsilon:.2f}\")\n",
        "            scores.append(time)\n",
        "            break\n",
        "\n",
        "    # Навчання:\n",
        "      # Зберегти досвід (remember).\n",
        "    if len(agent.memory) > BATCH_SIZE:\n",
        "        agent.replay(BATCH_SIZE)\n",
        "\n",
        "# %%\n",
        "# Plot training progress\n",
        "plt.plot(scores)\n",
        "plt.title('Training Progress')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Test the trained agent\n",
        "test_episodes = 10\n",
        "for e in range(test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):\n",
        "        env.render()  # Uncomment to visualize\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        if done:\n",
        "            print(f\"episode: {e}/{test_episodes}, score: {time}\")\n",
        "            break\n",
        "\n",
        "\n",
        "# %%\n",
        "# Plot training progress\n",
        "plt.plot(scores)\n",
        "plt.title('Training Progress')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Test the trained agent\n",
        "test_episodes = 10\n",
        "for e in range(test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):\n",
        "        env.render()  # Uncomment to visualize\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        if done:\n",
        "            print(f\"episode: {e}/{test_episodes}, score: {time}\")\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muz0M80dj5Ii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrCONggDXVJG"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Save the model\n",
        "agent.model.save(\"cartpole-dqn.keras\")  # Keras 3.x\n",
        "loaded_model = keras.models.load_model(\"cartpole-dqn.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtYUGHcbJNaX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vnev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
